{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP 1-Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment we generate 1000 sequences each comprising 10 SDRs generated at random. We present these sequences to the TM with learning \"on\". Each training epoch starts by shuffling the 1000 sequences and presenting each of them to the TM. During the simulation we keep track of spike trains from all cells. We use this data to estimate pairwise correlations among cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nupic.bindings.algorithms import TemporalMemory as TM\n",
    "from htmresearch.support.neural_correlations_utils import *\n",
    "    \n",
    "uintType = \"uint32\"\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "symbolsPerSequence = 10\n",
    "numSequences = 1000\n",
    "epochs = 10\n",
    "totalTS = epochs * numSequences * symbolsPerSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tm = TM(columnDimensions = (2048,),\n",
    "    cellsPerColumn=8,\n",
    "    initialPermanence=0.21,\n",
    "    connectedPermanence=0.3,\n",
    "    minThreshold=15,\n",
    "    maxNewSynapseCount=40,\n",
    "    permanenceIncrement=0.1,\n",
    "    permanenceDecrement=0.1,\n",
    "    activationThreshold=15,\n",
    "    predictedSegmentDecrement=0.01,\n",
    "    )\n",
    "\n",
    "sparsity = 0.02\n",
    "sparseCols = int(tm.numberOfColumns() * sparsity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed sequences to the TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create sequences\n",
    "allSequences = []\n",
    "for s in range(numSequences):\n",
    "    sequence = generateRandomSequence(symbolsPerSequence, tm.numberOfColumns(), sparsity)\n",
    "    allSequences.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "100 sequences processed\n",
      "200 sequences processed\n",
      "300 sequences processed\n",
      "400 sequences processed\n",
      "500 sequences processed\n",
      "600 sequences processed\n",
      "700 sequences processed\n",
      "800 sequences processed\n",
      "900 sequences processed\n",
      "\n",
      "Epoch: 1\n",
      "100 sequences processed\n",
      "200 sequences processed\n",
      "300 sequences processed\n",
      "400 sequences processed\n",
      "500 sequences processed\n",
      "600 sequences processed\n",
      "700 sequences processed\n",
      "800 sequences processed\n",
      "900 sequences processed\n",
      "\n",
      "Epoch: 2\n",
      "100 sequences processed\n",
      "200 sequences processed\n",
      "300 sequences processed\n",
      "400 sequences processed\n",
      "500 sequences processed\n",
      "600 sequences processed\n",
      "700 sequences processed\n",
      "800 sequences processed\n",
      "900 sequences processed\n",
      "\n",
      "Epoch: 3\n",
      "100 sequences processed\n",
      "200 sequences processed\n",
      "300 sequences processed\n",
      "400 sequences processed\n",
      "500 sequences processed\n",
      "600 sequences processed\n",
      "700 sequences processed\n",
      "800 sequences processed\n",
      "900 sequences processed\n",
      "\n",
      "Epoch: 4\n",
      "100 sequences processed\n",
      "200 sequences processed\n",
      "300 sequences processed\n",
      "400 sequences processed\n",
      "500 sequences processed\n",
      "600 sequences processed\n",
      "700 sequences processed\n",
      "800 sequences processed\n",
      "900 sequences processed\n",
      "\n",
      "Epoch: 5\n",
      "100 sequences processed\n",
      "200 sequences processed\n",
      "300 sequences processed\n",
      "400 sequences processed\n",
      "500 sequences processed\n",
      "600 sequences processed\n",
      "700 sequences processed\n",
      "800 sequences processed\n",
      "900 sequences processed\n",
      "\n",
      "Epoch: 6\n",
      "100 sequences processed\n",
      "200 sequences processed\n",
      "300 sequences processed\n",
      "400 sequences processed\n",
      "500 sequences processed\n",
      "600 sequences processed\n",
      "700 sequences processed\n",
      "800 sequences processed\n",
      "900 sequences processed\n",
      "\n",
      "Epoch: 7\n",
      "100 sequences processed\n",
      "200 sequences processed\n",
      "300 sequences processed\n",
      "400 sequences processed\n",
      "500 sequences processed\n",
      "600 sequences processed\n",
      "700 sequences processed\n",
      "800 sequences processed\n",
      "900 sequences processed\n",
      "\n",
      "Epoch: 8\n",
      "100 sequences processed\n",
      "200 sequences processed\n",
      "300 sequences processed\n",
      "400 sequences processed\n",
      "500 sequences processed\n",
      "600 sequences processed\n",
      "700 sequences processed\n",
      "800 sequences processed\n",
      "900 sequences processed\n",
      "\n",
      "Epoch: 9\n",
      "100 sequences processed\n",
      "200 sequences processed\n",
      "300 sequences processed\n",
      "400 sequences processed\n",
      "500 sequences processed\n",
      "600 sequences processed\n",
      "700 sequences processed\n",
      "800 sequences processed\n",
      "900 sequences processed\n",
      "*** DONE ***\n"
     ]
    }
   ],
   "source": [
    "spikeTrains = np.zeros((tm.numberOfCells(), totalTS), dtype = \"uint32\")\n",
    "columnUsage = np.zeros(tm.numberOfColumns(), dtype=\"uint32\")\n",
    "spikeCount = np.zeros(totalTS, dtype=\"uint32\")\n",
    "ts = 0\n",
    "\n",
    "entropyX = []\n",
    "entropyY = []\n",
    "\n",
    "negPCCX_cells = []\n",
    "negPCCY_cells = []\n",
    "\n",
    "numSpikesX = []\n",
    "numSpikesY = []\n",
    "\n",
    "numSpikes = 0\n",
    "\n",
    "negPCCX_cols = []\n",
    "negPCCY_cols = []\n",
    "\n",
    "traceX = []\n",
    "traceY = []\n",
    "\n",
    "# Randomly generate the indices of the columns to keep track during simulation time\n",
    "colIndicesLarge = np.random.permutation(tm.numberOfColumns())[0:125] # keep track of 125 columns = 1000 cells\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # shuffle sequences\n",
    "    print \"\"\n",
    "    print \"Epoch: \" + str(epoch)\n",
    "    seqIndices = np.random.permutation(np.arange(numSequences))\n",
    "    \n",
    "    for s in range(numSequences):\n",
    "        if s > 0 and s % 100 == 0:\n",
    "            print str(s) + \" sequences processed\"\n",
    "        for symbol in range(symbolsPerSequence):\n",
    "            tm.compute(allSequences[seqIndices[s]][symbol], learn=True)\n",
    "            \n",
    "            for cell in tm.getActiveCells():\n",
    "                spikeTrains[cell, ts] = 1\n",
    "                numSpikes += 1\n",
    "                spikeCount[ts] += 1\n",
    "                \n",
    "            # Obtain active columns:\n",
    "            activeColumnsIndices = [tm.columnForCell(i) for i in tm.getActiveCells()]\n",
    "            currentColumns = [1 if i in activeColumnsIndices else 0 for i in range(tm.numberOfColumns())]\n",
    "            for col in np.nonzero(currentColumns)[0]:\n",
    "                columnUsage[col] += 1\n",
    "                \n",
    "            if ts > 0 and ts % int(totalTS * 0.1) == 0:\n",
    "                numSpikesX.append(ts)\n",
    "                numSpikesY.append(numSpikes)            \n",
    "                numSpikes = 0\n",
    "                \n",
    "                #print \"++ Analyzing correlations (cells at random) ++\"                \n",
    "                subSpikeTrains = subSample(spikeTrains, 1000, tm.numberOfCells(), ts, 1000)\n",
    "                (corrMatrix, numNegPCC) = computePWCorrelations(subSpikeTrains, removeAutoCorr=True)\n",
    "                negPCCX_cells.append(ts)\n",
    "                negPCCY_cells.append(numNegPCC)                \n",
    "\n",
    "                traceX.append(ts)\n",
    "                #traceY.append(sum(1 for i in corrMatrix.ravel() if i > 0.5))\n",
    "                #traceY.append(np.std(corrMatrix))\n",
    "                #traceY.append(sum(1 for i in corrMatrix.ravel() if i > -0.05 and i < 0.1))\n",
    "                traceY.append(sum(1 for i in corrMatrix.ravel() if i > 0.0))\n",
    "\n",
    "                bins = 300\n",
    "                plt.hist(corrMatrix.ravel(), bins, alpha=0.5)                \n",
    "                plt.xlim(-0.05,0.1)\n",
    "                plt.xlabel(\"PCC\")\n",
    "                plt.ylabel(\"Frequency\")\n",
    "                plt.savefig(\"cellsHist_\" + str(ts))\n",
    "                plt.close()\n",
    "                entropyX.append(ts)\n",
    "                entropyY.append(computeEntropy(subSpikeTrains))                \n",
    "\n",
    "                #print \"++ Analyzing correlations (whole columns) ++\"\n",
    "                ### First the LARGE subsample of columns:\n",
    "                subSpikeTrains = subSampleWholeColumn(spikeTrains, colIndicesLarge, tm.getCellsPerColumn(), ts, 1000)\n",
    "                (corrMatrix, numNegPCC) = computePWCorrelationsWithinCol(subSpikeTrains, True, tm.getCellsPerColumn())\n",
    "                negPCCX_cols.append(ts)\n",
    "                negPCCY_cols.append(numNegPCC)                \n",
    "                #print \"++ Generating histogram ++\"\n",
    "                plt.hist(corrMatrix.ravel(), alpha=0.5)\n",
    "                plt.xlabel(\"PCC\")\n",
    "                plt.ylabel(\"Frequency\")\n",
    "                plt.savefig(\"colsHist_\" + str(ts))\n",
    "                plt.close()                 \n",
    "                \n",
    "            ts += 1\n",
    "            \n",
    "print \"*** DONE ***\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(traceX, traceY)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Positive PCC Count\")\n",
    "plt.savefig(\"positivePCCTrace\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparsityTraceX = []\n",
    "sparsityTraceY = []\n",
    "for i in range(totalTS - 1000):\n",
    "    sparsityTraceX.append(i)\n",
    "    sparsityTraceY.append(np.mean(spikeCount[i:1000 + i]) / tm.numberOfCells())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(sparsityTraceX, sparsityTraceY)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Sparsity\")\n",
    "plt.savefig(\"sparsityTrace\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot trace of negative PCCs\n",
    "plt.plot(negPCCX_cells, negPCCY_cells)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Negative PCC Count\")\n",
    "plt.savefig(\"negPCCTrace_cells\")\n",
    "plt.close()\n",
    "\n",
    "plt.plot(negPCCX_cols, negPCCY_cols)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Negative PCC Count\")\n",
    "plt.savefig(\"negPCCTrace_cols\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(numSpikesX, numSpikesY)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Num Spikes\")\n",
    "plt.savefig(\"numSpikesTrace\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot entropy\n",
    "plt.plot(entropyX, entropyY)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.savefig(\"entropyTM\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(columnUsage)\n",
    "plt.xlabel(\"Number of times active\")\n",
    "plt.ylabel(\"Number of columns\")\n",
    "plt.savefig(\"columnUsage\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISI analysis (with Poisson model too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subSpikeTrains = subSample(spikeTrains, 1000, tm.numberOfCells(), 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "isi = computeISI(subSpikeTrains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print ISI distribution of TM\n",
    "bins = 100\n",
    "plt.hist(isi, bins)\n",
    "plt.xlim(0,1000)\n",
    "# plt.xlim(89500,92000)\n",
    "plt.xlabel(\"ISI\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"isiTM\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.mean(isi)\n",
    "print np.std(isi)\n",
    "print np.std(isi)/np.mean(isi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate spike distribution\n",
    "spikeCount = []\n",
    "for cell in range(np.shape(subSpikeTrains)[0]):\n",
    "    spikeCount.append(np.count_nonzero(subSpikeTrains[cell,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = 25\n",
    "plt.hist(spikeCount, bins)\n",
    "plt.xlabel(\"Spike Count\")\n",
    "plt.ylabel(\"Number of cells\")\n",
    "plt.savefig(\"spikesHist_TM\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#firingRate = 18\n",
    "firingRate = np.mean(subSpikeTrains) * 1000\n",
    "print \"firing rate: \" + str(firingRate)\n",
    "pSpikeTrain = poissonSpikeGenerator(int(firingRate),np.shape(subSpikeTrains)[1],np.shape(subSpikeTrains)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "isi = computeISI(pSpikeTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print ISI distribution of Poisson model\n",
    "\n",
    "#bins = np.linspace(np.min(isi), np.max(isi), 50)\n",
    "bins = 100\n",
    "plt.hist(isi, bins)\n",
    "plt.xlim(0,600)\n",
    "# plt.xlim(89500,92000)\n",
    "plt.xlabel(\"ISI\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"isiPOI\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.mean(isi)\n",
    "print np.std(isi)\n",
    "print np.std(isi)/np.mean(isi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate spike distribution\n",
    "spikeCount = []\n",
    "for cell in range(np.shape(pSpikeTrain)[0]):\n",
    "    spikeCount.append(np.count_nonzero(pSpikeTrain[cell,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = 25\n",
    "plt.hist(spikeCount, bins)\n",
    "plt.xlabel(\"Spike Count\")\n",
    "plt.ylabel(\"Number of cells\")\n",
    "plt.savefig(\"spikesHist_POI\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raster Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subSpikeTrains = subSample(spikeTrains, 100, tm.numberOfCells(), -1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rasterPlot(subSpikeTrains, \"TM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pSpikeTrain = poissonSpikeGenerator(firingRate,np.shape(subSpikeTrains)[1],np.shape(subSpikeTrains)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rasterPlot(pSpikeTrain, \"Poisson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Accuracy Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simpleAccuracyTest(\"random\", tm, allSequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elad Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample from both TM_SpikeTrains and Poisson_SpikeTrains. 10 cells for 1000 (?) timesteps\n",
    "wordLength = 10\n",
    "firingRate = np.mean(subSpikeTrains) * 1000 \n",
    "\n",
    "# generate all 2^N strings:\n",
    "binaryStrings = list(itertools.product([0, 1], repeat=wordLength))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trials = 10\n",
    "\n",
    "x = [] #observed\n",
    "y = [] #predicted by random model\n",
    "\n",
    "for t in range(trials):\n",
    "    print \"Trial: \" + str(t)\n",
    "    # sample from spike trains\n",
    "    subSpikeTrains = subSample(spikeTrains, wordLength, tm.numberOfCells(), 0, 0)\n",
    "    pSpikeTrain = poissonSpikeGenerator(firingRate,np.shape(subSpikeTrains)[1],np.shape(subSpikeTrains)[0])    \n",
    "    for i in range(2**wordLength):\n",
    "        if i == 0:\n",
    "            continue\n",
    "#         if i % 100 == 0:\n",
    "#             print str(i) + \" words processed\"\n",
    "        binaryWord = np.array(binaryStrings[i], dtype=\"uint32\")\n",
    "        x.append(countInSample(binaryWord, subSpikeTrains))\n",
    "        y.append(countInSample(binaryWord, pSpikeTrain))\n",
    "#     print \"**All words processed**\"    \n",
    "#     print \"\"\n",
    "print \"*** DONE ***\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.loglog(x, y, 'bo',basex=10)\n",
    "plt.xlabel(\"Observed\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.plot(x,x,'k-')\n",
    "plt.xlim(0,np.max(x))\n",
    "plt.savefig(\"EladPlot\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Save TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saveTM(tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to load the TM back from the file do:\n",
    "with open('tm.nta', 'rb') as f:\n",
    "    proto2 = TemporalMemoryProto_capnp.TemporalMemoryProto.read(f, traversal_limit_in_words=2**61)\n",
    "tm = TM.read(proto2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overlapMatrix = inputAnalysis(allSequences, \"random\", tm.numberOfColumns())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show heatmap of overlap matrix\n",
    "plt.imshow(overlapMatrix, cmap='spectral', interpolation='nearest')\n",
    "cb = plt.colorbar()\n",
    "cb.set_label('Overlap Score')\n",
    "plt.savefig(\"overlapScore_heatmap\")\n",
    "plt.close()\n",
    "# plt.show()\n",
    "\n",
    "# generate histogram\n",
    "bins = 60\n",
    "(n, bins, patches) = plt.hist(overlapMatrix.ravel(), bins, alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"Overlap Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"overlapScore_hist\")\n",
    "\n",
    "plt.xlim(0,0.15)\n",
    "plt.xlabel(\"Overlap Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"overlapScore_hist_ZOOM\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "trials = 1000\n",
    "for t in range(trials):\n",
    "    pSpikeTrain = poissonSpikeGenerator(18,1000,1)\n",
    "    x.append(np.count_nonzero(pSpikeTrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = 25\n",
    "plt.hist(x, bins)\n",
    "plt.savefig(\"test_spikePOI\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
