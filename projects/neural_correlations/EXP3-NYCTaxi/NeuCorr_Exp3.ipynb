{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP 3-NYCtaxi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment we use the NYC taxi dataset. We pass it through a scalar encoder, spatial pooler and then to the TM. We do a single pass to the TM and keep track of the spike trains of each cell. We use this data to estimate pairwise correlations among cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from nupic.encoders import ScalarEncoder\n",
    "from nupic.bindings.algorithms import TemporalMemory as TM\n",
    "from nupic.bindings.algorithms import SpatialPooler as SP\n",
    "from htmresearch.support.neural_correlations_utils import *\n",
    "\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputSize = 109\n",
    "maxItems = 17520\n",
    "tmEpochs = 1\n",
    "totalTS = maxItems * tmEpochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read csv file\n",
    "df = pd.read_csv('nyc_taxi.csv', skiprows=[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tm = TM(columnDimensions = (2048,),\n",
    "        cellsPerColumn=8, # We changed here the number of cells per col, initially they were 32\n",
    "        initialPermanence=0.21,\n",
    "        connectedPermanence=0.3,\n",
    "        minThreshold=15,\n",
    "        maxNewSynapseCount=40,\n",
    "        permanenceIncrement=0.1,\n",
    "        permanenceDecrement=0.1,\n",
    "        activationThreshold=15,\n",
    "        predictedSegmentDecrement=0.01\n",
    "       )\n",
    "\n",
    "sparsity = 0.02\n",
    "sparseCols = int(tm.numberOfColumns() * sparsity)\n",
    "\n",
    "sp = SP(inputDimensions=(inputSize,),\n",
    "        columnDimensions=(2048,),\n",
    "        potentialRadius = int(0.5*inputSize),\n",
    "        numActiveColumnsPerInhArea = sparseCols,\n",
    "        potentialPct = 0.9,\n",
    "        globalInhibition = True,\n",
    "        synPermActiveInc = 0.0001,\n",
    "        synPermInactiveDec = 0.0005,\n",
    "        synPermConnected = 0.5,\n",
    "        maxBoost = 1.0,\n",
    "        spVerbosity = 1\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I. Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 items processed\n",
      "1000 items processed\n",
      "1500 items processed\n",
      "2000 items processed\n",
      "2500 items processed\n",
      "3000 items processed\n",
      "3500 items processed\n",
      "4000 items processed\n",
      "4500 items processed\n",
      "5000 items processed\n",
      "5500 items processed\n",
      "6000 items processed\n",
      "6500 items processed\n",
      "7000 items processed\n",
      "7500 items processed\n",
      "8000 items processed\n",
      "8500 items processed\n",
      "9000 items processed\n",
      "9500 items processed\n",
      "10000 items processed\n",
      "10500 items processed\n",
      "11000 items processed\n",
      "11500 items processed\n",
      "12000 items processed\n",
      "12500 items processed\n",
      "13000 items processed\n",
      "13500 items processed\n",
      "14000 items processed\n",
      "14500 items processed\n",
      "15000 items processed\n",
      "15500 items processed\n",
      "16000 items processed\n",
      "16500 items processed\n",
      "17000 items processed\n",
      "17500 items processed\n",
      "*** All items encoded! ***\n"
     ]
    }
   ],
   "source": [
    "rawValues = []\n",
    "remainingRows = maxItems\n",
    "numTrainingItems = 15000\n",
    "trainSet = []\n",
    "nonTrainSet = []\n",
    "\n",
    "se = ScalarEncoder(n=109, w=29, minval=0, maxval=40000, clipInput=True)\n",
    "s = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if s > 0 and s % 500 == 0:\n",
    "        print str(s) + \" items processed\"\n",
    "        \n",
    "    rawValues.append(row['passenger_count'])\n",
    "    \n",
    "    if s < numTrainingItems:\n",
    "        trainSet.append(se.encode(row['passenger_count']))\n",
    "    else:\n",
    "        nonTrainSet.append(se.encode(row['passenger_count']))\n",
    "        \n",
    "    remainingRows -= 1\n",
    "    s += 1\n",
    "    if remainingRows == 0: \n",
    "        break\n",
    "print \"*** All items encoded! ***\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II. Spatial Pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 0\n",
      "Training epoch: 1\n",
      "Training epoch: 2\n",
      "Training epoch: 3\n",
      "500 items processed\n",
      "1000 items processed\n",
      "1500 items processed\n",
      "2000 items processed\n",
      "2500 items processed\n",
      "*** All items processed! ***\n"
     ]
    }
   ],
   "source": [
    "allSequences = []\n",
    "outputColumns = np.zeros(sp.getNumColumns(), dtype=\"uint32\")\n",
    "columnUsage = np.zeros(sp.getNumColumns(), dtype=\"uint32\")\n",
    "\n",
    "# Set epochs for spatial-pooling:\n",
    "spEpochs = 4\n",
    "\n",
    "for epoch in range(spEpochs):\n",
    "    print \"Training epoch: \" + str(epoch)\n",
    "    \n",
    "    #randomize records in training set\n",
    "    randomIndex = np.random.permutation(np.arange(numTrainingItems))\n",
    "    \n",
    "    for i in range(numTrainingItems):\n",
    "        sp.compute(trainSet[randomIndex[i]], True, outputColumns)\n",
    "        # Populate array for Yuwei plot:\n",
    "        for col in outputColumns.nonzero():\n",
    "            columnUsage[col] += 1                        \n",
    "        if epoch == (spEpochs - 1):\n",
    "            allSequences.append(outputColumns.nonzero()) \n",
    "\n",
    "for i in range(maxItems - numTrainingItems):\n",
    "    if i > 0 and i % 500 == 0:\n",
    "        print str(i) + \" items processed\"    \n",
    "    sp.compute(nonTrainSet[i], False, outputColumns)\n",
    "    allSequences.append(outputColumns.nonzero())\n",
    "    # Populate array for Yuwei plot:\n",
    "    for col in outputColumns.nonzero():\n",
    "        columnUsage[col] += 1                \n",
    "\n",
    "print \"*** All items processed! ***\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = 50\n",
    "plt.hist(columnUsage, bins)\n",
    "plt.xlabel(\"Number of times active\")\n",
    "plt.ylabel(\"Number of columns\")\n",
    "plt.savefig(\"columnUsage_SP\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III. Temporal Memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "0 items processed\n",
      "1000 items processed\n",
      "2000 items processed\n",
      "3000 items processed\n",
      "4000 items processed\n",
      "5000 items processed\n",
      "6000 items processed\n",
      "7000 items processed\n",
      "8000 items processed\n",
      "9000 items processed\n",
      "10000 items processed\n",
      "11000 items processed\n",
      "12000 items processed\n",
      "13000 items processed\n",
      "14000 items processed\n",
      "15000 items processed\n",
      "16000 items processed\n",
      "17000 items processed\n",
      "*** DONE ***\n"
     ]
    }
   ],
   "source": [
    "spikeTrains = np.zeros((tm.numberOfCells(), totalTS), dtype = \"uint32\")\n",
    "columnUsage = np.zeros(tm.numberOfColumns(), dtype=\"uint32\")\n",
    "spikeCount = np.zeros(totalTS, dtype=\"uint32\")\n",
    "ts = 0\n",
    "\n",
    "entropyX = []\n",
    "entropyY = []\n",
    "\n",
    "negPCCX_cells = []\n",
    "negPCCY_cells = []\n",
    "\n",
    "numSpikesX = []\n",
    "numSpikesY = []\n",
    "\n",
    "numSpikes = 0\n",
    "\n",
    "negPCCX_cols = []\n",
    "negPCCY_cols = []\n",
    "\n",
    "traceX = []\n",
    "traceY = []\n",
    "\n",
    "# Randomly generate the indices of the columns to keep track during simulation time\n",
    "# keep track of 125 columns = 1000 cells\n",
    "#colIndicesLarge = np.random.permutation(tm.numberOfColumns())[0:125] \n",
    "\n",
    "for e in range(tmEpochs):\n",
    "    print \"\"\n",
    "    print \"Epoch: \" + str(e)\n",
    "    for s in range(maxItems):\n",
    "        if s % 1000 == 0:\n",
    "            print str(s) + \" items processed\"\n",
    "        \n",
    "        tm.compute(allSequences[s][0].tolist(), learn=True)\n",
    "        for cell in tm.getActiveCells():\n",
    "            spikeTrains[cell, ts] = 1 \n",
    "            numSpikes += 1\n",
    "            spikeCount[ts] += 1\n",
    "            \n",
    "        # Obtain active columns:\n",
    "        activeColumnsIndices = [tm.columnForCell(i) for i in tm.getActiveCells()]\n",
    "        currentColumns = [1 if i in activeColumnsIndices else 0 for i in range(tm.numberOfColumns())]\n",
    "        for col in np.nonzero(currentColumns)[0]:\n",
    "            columnUsage[col] += 1                \n",
    "    \n",
    "        if ts > 0 and ts % int(totalTS * 0.1) == 0:\n",
    "            numSpikesX.append(ts)\n",
    "            numSpikesY.append(numSpikes)\n",
    "            \n",
    "            numSpikes = 0            \n",
    "            subSpikeTrains = subSample(spikeTrains, 1000, tm.numberOfCells(), ts, 1000)\n",
    "            (corrMatrix, numNegPCC) = computePWCorrelations(subSpikeTrains, removeAutoCorr=True)\n",
    "            negPCCX_cells.append(ts)\n",
    "            negPCCY_cells.append(numNegPCC)                \n",
    "            bins = 300\n",
    "            plt.hist(corrMatrix.ravel(), bins, alpha=0.5)                \n",
    "            plt.xlim(-0.1,0.2)\n",
    "            plt.xlabel(\"PCC\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.savefig(\"cellsHist\" + str(ts))\n",
    "            plt.close()\n",
    "            \n",
    "            traceX.append(ts)\n",
    "            #traceY.append(sum(1 for i in corrMatrix.ravel() if i > 0.5))\n",
    "            #traceY.append(np.std(corrMatrix))\n",
    "            #traceY.append(sum(1 for i in corrMatrix.ravel() if i > -0.05 and i < 0.1))\n",
    "            traceY.append(sum(1 for i in corrMatrix.ravel() if i > 0.0))\n",
    "            \n",
    "            entropyX.append(ts)\n",
    "            entropyY.append(computeEntropy(subSpikeTrains))\n",
    "\n",
    "            #print \"++ Analyzing correlations (whole columns) ++\"\n",
    "            ### First the LARGE subsample of columns:\n",
    "            colIndicesLarge = np.random.permutation(tm.numberOfColumns())[0:125] \n",
    "            \n",
    "            subSpikeTrains = subSampleWholeColumn(spikeTrains, colIndicesLarge, tm.getCellsPerColumn(), ts, 1000)\n",
    "            (corrMatrix, numNegPCC) = computePWCorrelationsWithinCol(subSpikeTrains, True, tm.getCellsPerColumn())\n",
    "            negPCCX_cols.append(ts)\n",
    "            negPCCY_cols.append(numNegPCC)                \n",
    "            #print \"++ Generating histogram ++\"\n",
    "            plt.hist(corrMatrix.ravel(), alpha=0.5)\n",
    "            plt.xlabel(\"PCC\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.savefig(\"colsHist_\" + str(ts))\n",
    "            plt.close()                 \n",
    "\n",
    "        ts += 1                \n",
    "        \n",
    "print \"*** DONE ***\"\n",
    "# end for-epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(traceX, traceY)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Positive PCC Count\")\n",
    "plt.savefig(\"positivePCCTrace\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparsityTraceX = []\n",
    "sparsityTraceY = []\n",
    "for i in range(totalTS - 1000):\n",
    "    sparsityTraceX.append(i)\n",
    "    sparsityTraceY.append(np.mean(spikeCount[i:1000 + i]) / tm.numberOfCells())\n",
    "\n",
    "plt.plot(sparsityTraceX, sparsityTraceY)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Sparsity\")\n",
    "plt.savefig(\"sparsityTrace\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot trace of negative PCCs\n",
    "plt.plot(negPCCX_cells, negPCCY_cells)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Negative PCC Count\")\n",
    "plt.savefig(\"negPCCTrace_cells\")\n",
    "plt.close()\n",
    "\n",
    "plt.plot(negPCCX_cols, negPCCY_cols)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Negative PCC Count\")\n",
    "plt.savefig(\"negPCCTrace_cols\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print computeEntropy()\n",
    "plt.plot(entropyX, entropyY)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.savefig(\"entropyTM\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = 50\n",
    "plt.hist(columnUsage, bins)\n",
    "plt.xlabel(\"Number of times active\")\n",
    "plt.ylabel(\"Number of columns\")\n",
    "plt.savefig(\"columnUsage_TM\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(numSpikesX, numSpikesY)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Num Spikes\")\n",
    "plt.savefig(\"numSpikesTrace\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV. Analysis of Spike Trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simpleAccuracyTest(\"periodic\", tm, allSequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subSpikeTrains = subSample(spikeTrains, 1000, tm.numberOfCells(), 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "isi = computeISI(subSpikeTrains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#bins = np.linspace(np.min(isi), np.max(isi), 50)\n",
    "bins = 100\n",
    "plt.hist(isi, bins)\n",
    "# plt.xlim(0,4000)\n",
    "# plt.xlim(89500,92000)\n",
    "plt.xlabel(\"ISI\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"isiTM\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.mean(isi)\n",
    "print np.std(isi)\n",
    "print np.std(isi)/np.mean(isi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raster plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subSpikeTrains = subSample(spikeTrains, 100, tm.numberOfCells(), -1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rasterPlot(subSpikeTrains, \"TM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part V. Save TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saveTM(tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to load the TM back from the file do:\n",
    "with open('tm.nta', 'rb') as f:\n",
    "    proto2 = TemporalMemoryProto_capnp.TemporalMemoryProto.read(f, traversal_limit_in_words=2**61)\n",
    "tm = TM.read(proto2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part VI. Analysis of Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overlapMatrix = inputAnalysis(allSequences, \"periodic\", tm.numberOfColumns())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# show heatmap of overlap matrix\n",
    "plt.imshow(overlapMatrix, cmap='spectral', interpolation='nearest')\n",
    "cb = plt.colorbar()\n",
    "cb.set_label('Overlap Score')\n",
    "plt.savefig(\"overlapScore_heatmap\")\n",
    "plt.close()\n",
    "# plt.show()\n",
    "\n",
    "# generate histogram\n",
    "bins = 60\n",
    "(n, bins, patches) = plt.hist(overlapMatrix.ravel(), bins, alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"Overlap Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"overlapScore_hist\")\n",
    "\n",
    "plt.xlim(0.2,1)\n",
    "plt.ylim(0,1000000)\n",
    "plt.xlabel(\"Overlap Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"overlapScore_hist_ZOOM\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
